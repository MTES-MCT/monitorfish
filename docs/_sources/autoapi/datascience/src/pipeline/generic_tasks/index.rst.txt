:py:mod:`datascience.src.pipeline.generic_tasks`
================================================

.. py:module:: datascience.src.pipeline.generic_tasks


Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   datascience.src.pipeline.generic_tasks.extract
   datascience.src.pipeline.generic_tasks.load



.. py:function:: extract(db_name: str, query_filepath: Union[pathlib.Path, str], dtypes: Union[None, dict] = None, parse_dates: Union[list, dict, None] = None, params=None) -> pandas.DataFrame

   Run SQL query against the indicated database and return the result as a
   `pandas.DataFrame`.

   :param db_name: name of the databse to extract from : "fmc", "ocan",
                   "monitorfish_local" or "monitorfish_remote"
   :type db_name: str
   :param query_filepath: path to .sql file, starting from the saved
                          queries folder. example : "ocan/nav_fr_peche.sql"
   :type query_filepath: Union[Path, str]
   :param dtypes: If specified, use {col: dtype, …}, where
                  col is a column label and dtype is a numpy.dtype or Python type to cast
                  one or more of the DataFrame’s columns to column-specific types.
                  Defaults to None.
   :type dtypes: Union[None, dict], optional
   :param parse_dates:
                       - List of column names to parse as dates.
                       - Dict of ``{column_name: format string}`` where format string is
                       strftime compatible in case of parsing string times or is one of
                       (D, s, ns, ms, us) in case of parsing integer timestamps.
                       - Dict of ``{column_name: arg dict}``, where the arg dict corresponds
                       to the keyword arguments of :func:`pandas.to_datetime`

                       Defaults to None.
   :type parse_dates: Union[list, dict, None], optional

   :returns: [description]
   :rtype: pd.DataFrame


.. py:function:: load(df: pandas.DataFrame, table_name: str, schema: str, db_name: str, logger: Union[None, logging.Logger], delete_before_insert: bool = False, pg_array_columns: Union[None, list] = None, handle_array_conversion_errors: bool = True, value_on_array_conversion_error='{}', jsonb_columns: Union[None, list] = None)


