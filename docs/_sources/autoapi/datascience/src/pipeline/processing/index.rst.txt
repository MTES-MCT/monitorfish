:py:mod:`datascience.src.pipeline.processing`
=============================================

.. py:module:: datascience.src.pipeline.processing


Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   datascience.src.pipeline.processing.is_a_value
   datascience.src.pipeline.processing.concatenate_values
   datascience.src.pipeline.processing.concatenate_columns
   datascience.src.pipeline.processing.combine_overlapping_columns
   datascience.src.pipeline.processing.get_first_non_null_column_name
   datascience.src.pipeline.processing.df_to_dict_series
   datascience.src.pipeline.processing.zeros_ones_to_bools
   datascience.src.pipeline.processing.to_pgarr
   datascience.src.pipeline.processing.df_values_to_psql_arrays
   datascience.src.pipeline.processing.json_converter
   datascience.src.pipeline.processing.to_json
   datascience.src.pipeline.processing.df_values_to_json
   datascience.src.pipeline.processing.drop_rows_already_in_table
   datascience.src.pipeline.processing.prepare_df_for_loading
   datascience.src.pipeline.processing.join_on_multiple_keys



.. py:function:: is_a_value(x) -> bool

   Returns False if pd.isna(x), True otherwise.

   NB : The same result could be obtained simply by checking pd.isna(x),
   but checking if x is None before checking pd.isna(x)
   improves performance on DataFrames containing many None values,
   since checking pd.isna(x) is slower than checking if x is None.

   :param x: Anything

   :returns: False if pd.isna(x), True otherwise
   :rtype: bool


.. py:function:: concatenate_values(row: pandas.Series) -> List

   Filters the input pandas Series to keep only distinct non null values
   and returns the result as a python list.

   :param row: pandas Series
   :type row: pd.Series

   :returns: list of distinct non null values in row
   :rtype: List


.. py:function:: concatenate_columns(df: pandas.DataFrame, input_col_names: List) -> pandas.Series

   For each row in the input DataFrame, the distinct and non null values contained in
   the columns input_col_names are stored in a list. A pandas Series of the same length
   as the input DataFrame is then constructed with these lists as values.

   :param df: input DataFrame
   :type df: pd.DataFrame
   :param input_col_names: the names of the columns to use
   :type input_col_names: List

   :returns: resulting Series
   :rtype: pd.Series


.. py:function:: combine_overlapping_columns(df: pandas.DataFrame, ordered_cols_list: List) -> pandas.Series

   Combines several columns into one by taking the first_valid_value in each row,
   in the order of the ordered_cols_list.

   Returns a pandas Series with the combined results.

   :param df: input pandas DataFrame
   :type df: pd.DataFrame
   :param ordered_cols_list: list of column names
   :type ordered_cols_list: List

   :returns:

             Series containing the first valid value in each row of the DataFrame,
                 taken in the ordered_cols_list columns.
   :rtype: pd.Series


.. py:function:: get_first_non_null_column_name(df: pandas.DataFrame, result_labels: Union[None, dict] = None) -> pandas.Series

   Returns a Series with the same index as the input DataFrame, whose values are
   the name of the first column (or the corresponding label, if provided) with a
   non-null value in each row, from left to right.

   Rows with all null values return None.

   :param df: input pandas DataFrame
   :type df: pd.DataFrame
   :param result_labels: if provided, must be a mapping of column names to the
   :type result_labels: dict
   :param corresponding labels in the result.:

   :returns: Series containing the name of the first column with a non-null value
             in each row of the DataFrame, from left to right
   :rtype: pd.Series


.. py:function:: df_to_dict_series(df: pandas.DataFrame, result_colname: str = 'json_col')

   Converts a pandas DataFrame into a Series with the same index as the input
   DataFrame and whose values are dictionnaries like :

       {'column_1' : value, 'column_2': value}

   :param df: input DataFrame
   :type df: pd.DataFrame
   :param result_colname: optionnal, name of result Series
   :type result_colname: Union[str, None]

   :returns: pandas Series
   :rtype: pd.Series


.. py:function:: zeros_ones_to_bools(df: pandas.DataFrame) -> pandas.DataFrame

   Converts a pandas DataFrame containing "0", "1" and None values
   to a DataFrame with False, True and None values respectively.

   Useful to convert boolean data extracted from Oracle databases, since Oracle does
   not have a boolean data type and boolean data is often stored as "0"s and "1"s.


.. py:function:: to_pgarr(x: Union[list, set, numpy.ndarray], handle_errors: bool = False, value_on_error: Union[str, None] = None) -> Union[str, None]

   Converts a python `list`, `set` or `numpy.ndarray` to a string with Postgresql
   array syntax.

   Elements of the list-like input argument are converted to `string` type, then
   stripped of leading and trailing blank spaces, and finally filtered to keep only
   non empty strings.

   This transformation is required on the elements of a DataFrame's columns that
   contain collections before bulk inserting the DataFrame into Postgresql with
   the psql_insert_copy method.

   :param x: iterable to serialize as Postgres array
   :type x: list, set or numpy.ndarray
   :param handle_errors: if ``True``, returns ``value_on_error`` instead of raising
                         ``ValueError`` when the input is of an unexpected type
   :type handle_errors: bool
   :param value_on_error: value to return on errors, if ``handle_errors``
                          is ``True``
   :type value_on_error: str or None

   :returns: string with Postgresql Array compatible syntax
   :rtype: str

   :raises ValueError: when ``handle_errors`` is False and ``x`` is not list-like.

   .. rubric:: Examples

   >>> to_pgarr([1, 2, "a ", "b", "", " "])
   "{1,2,'a','b'}"
   >>> to_pgarr(None)
   ValueError

   >>> to_pgarr(None, handle_errors=True, value_on_error="{}")
   "{}"

   >>> to_pgarr(np.nan, handle_errors=True, value_on_error=None)


.. py:function:: df_values_to_psql_arrays(df: pandas.DataFrame, handle_errors: bool = False, value_on_error: Union[str, None] = None) -> pandas.DataFrame

   Returns a `pandas.DataFrame` with all values serialized as strings
   with Postgresql array syntax. All values must be of type list, set or numpy array.
   Other values raise errors, which may be handled if handle_errors is set to True.

   See `to_pgarr` for details on error handling.

   This is required before bulk loading a pandas.DataFrame into a Postgresql table
   with the psql_insert_copy method.


   :param df: pandas DataFrame
   :type df: pd.DataFrame

   :returns:

             pandas DataFrame with the same shape and index, all values
                 serialized as strings with Postgresql array syntax.
   :rtype: pd.DataFrame

   Examples :

       >>> df_to_psql_arrays(pd.DataFrame({'a': [[1, 2], ['a', 'b']]}))
           a
       0   {1,2,3}
       1   {a,b}


.. py:function:: json_converter(x)

   Converter for types not natively handled by json.dumps


.. py:function:: to_json(x: Any) -> str

   Converts python object to json string.


.. py:function:: df_values_to_json(df: pandas.DataFrame) -> pandas.DataFrame

   Returns a `pandas.DataFrame` with all values serialized to json string.

   This is required before bulk loading into a Postgresql table with
   the psql_insert_copy method.

   See `to_json` function for details.

   :param df: pandas DataFrame
   :type df: pd.DataFrame

   :returns:

             pandas DataFrame with the same shape and index, all values
                 serialized as json strings.
   :rtype: pd.DataFrame


.. py:function:: drop_rows_already_in_table(df: pandas.DataFrame, df_column_name: str, table: sqlalchemy.Table, table_column_name: str, connection: sqlalchemy.engine.base.Connection, logger: logging.Logger) -> pandas.DataFrame

   Removes rows from the input DataFrame `df` in which the column `df_column_name`
   contains values that are already present in the column `table_column_name` of the
   table `table`, and returns the filtered DataFrame.


.. py:function:: prepare_df_for_loading(df: pandas.DataFrame, logger: logging.Logger, pg_array_columns: Union[None, list] = None, handle_array_conversion_errors: bool = True, value_on_array_conversion_error='{}', jsonb_columns: Union[None, list] = None)


.. py:function:: join_on_multiple_keys(left: pandas.DataFrame, right: pandas.DataFrame, on: list, how: str = 'inner')

   Join two pandas DataFrames, attempting to match rows on several keys by
   decreasing order of priority.

   Joins are performed successively with each of the keys listed in `on`, and results
   are then concatenated to form the final result. This is different from joining on a
   composite key where all keys must match simultaneously : here, rows of left and
   right DataFrames are joined if at least one of the keys match.

   Joins are performed on the keys listed in `on` by "decreasing order or priority" in
   the sense that rows of left and right that have been matched on one key are removed
   from ulterior joins perfomed on the next keys.

   During each of the joins on the individual keys, non-joining key pairs from left and
   right DataFrames are coalesced.

   :param left: pandas DataFrame
   :type left: pd.DataFrame
   :param right: pandas DataFrame
   :type right: pd.DataFrame
   :param on: list of column names to use as join keys
   :type on: list
   :param how: 'inner', 'left', 'right' or 'outer'. Defaults to 'inner'.
   :type how: str

   :returns: result of join operation
   :rtype: pd.DataFrame


